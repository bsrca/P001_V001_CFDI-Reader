{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "#Libriaries\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandasgui as pg \n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "import re\n",
    "import pyodbc \n",
    "import pydash\n",
    "import glob\n",
    "\n",
    "#Define the connection details\n",
    "server = 'jacobo-dev.database.windows.net'\n",
    "port = '1433'\n",
    "database = 'jacobo-dev-sqlserver-azure-001'\n",
    "username = 'azure-admin'\n",
    "password = 'ja-2023-un0ypzjo'\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "#Define the connection string\n",
    "conn_str = f\"DRIVER={driver};SERVER={server},{port};DATABASE={database};UID={username};PWD={password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\"\n",
    "\n",
    "#Test SQL DB Connection\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"Connection successful!\")\n",
    "    conn.close()\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error connecting to database:\", e)\n",
    "\n",
    "\n",
    "#defined functions\n",
    "def get_root(file_path):\n",
    "    \"\"\"Parses an XML file and returns its root element.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        xml.etree.ElementTree.Element: The root element of the XML file.\n",
    "    \"\"\"\n",
    "    xml_tree = ET.parse(file_path)\n",
    "    return xml_tree.getroot()\n",
    "\n",
    "\n",
    "def get_child_elements(xml_root):\n",
    "    \"\"\"Generates a list to store all child elements of a given xml element.\n",
    "    \n",
    "    Args:\n",
    "        xml_root (xml.etree.ElementTree.Element): The root element of the XML file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of all child elements.\n",
    "    \"\"\"\n",
    "    elements = [child for child in xml_root]\n",
    "    for child in xml_root:\n",
    "        elements.extend(get_child_elements(child))\n",
    "    # Append xml root to elements list\n",
    "    elements.append(xml_root)\n",
    "    return elements\n",
    "\n",
    "\n",
    "def get_attribute_cfdi(elements, tag_contains, attribute):\n",
    "    \"\"\"Extracts the attributes from xml elements based on the tag and attribute provided, and get back unique values.\n",
    "    \n",
    "    Args:\n",
    "        elements (list): List of XML elements.\n",
    "        tag_contains (str): Tag name to search for.\n",
    "        attribute (str): Attribute name to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: Comma-separated string of attribute values.\n",
    "    \"\"\"\n",
    "    attribute_values = set(element.attrib.get(attribute) \n",
    "                        for element in elements if tag_contains in element.tag \n",
    "                        and element.attrib.get(attribute) is not None)\n",
    "    return ', '.join(attribute_values)\n",
    "\n",
    "\n",
    "def get_attribute_cfdi_from_path(xml_path, tag_contains, attribute):\n",
    "    \"\"\"Extracts the attribute from the root xml based on the tag and attribute provided.\n",
    "    \n",
    "    Args:\n",
    "        xml_path (str): Path to the XML file.\n",
    "        tag_contains (str): Tag name to search for.\n",
    "        attribute (str): Attribute name to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: Comma-separated string of attribute values.\n",
    "    \"\"\"\n",
    "    xml_root = get_root(xml_path)\n",
    "    elements = get_child_elements(xml_root)\n",
    "    elements.append(xml_root)\n",
    "    return get_attribute_cfdi(elements, tag_contains, attribute)\n",
    "\n",
    "\n",
    "\n",
    "def pascal_case(text):\n",
    "    \"\"\"Convert a string into PascalCase.\"\"\"\n",
    "    return ''.join(word.capitalize() for word in text.split())\n",
    "\n",
    "\n",
    "def get_xml_metadata(elements):\n",
    "    \"\"\"Extracts the metadata from xml elements.\n",
    "    \n",
    "    Args:\n",
    "        elements (list): List of XML elements.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing XML metadata.\n",
    "    \"\"\"\n",
    "    # Define a list to store row data and a dictionary for the attribute field counters\n",
    "    data, field_counters = [], {}\n",
    "\n",
    "    # Get UUID from 'TimbreFiscalDigital' element\n",
    "    uuid = get_attribute_cfdi(elements, 'TimbreFiscalDigital', 'UUID')\n",
    "    \n",
    "    # Iterate over elements\n",
    "    for element in elements:\n",
    "        if element.attrib:\n",
    "            cleaned_tag = pascal_case(re.sub(r\"{.*?}\", \"\", element.tag)).lower()\n",
    "            file_name = os.path.basename(xml_file_path)\n",
    "            file_path = xml_file_path\n",
    "\n",
    "            # If this tag has not been seen before, initialize its counter\n",
    "            if element.tag not in field_counters:\n",
    "                field_counters[element.tag] = 1\n",
    "\n",
    "            # Iterate over attributes of the element and add the data to the list\n",
    "            for key, value in element.attrib.items():\n",
    "                cleaned_key = pascal_case(re.sub(r\"{.*?}\", \"\", key)).lower()\n",
    "                data.append({'field_number': field_counters[element.tag], 'file_path': file_path, 'file_name': file_name, 'cleaned_tag': cleaned_tag, 'tag': element.tag, 'key': key, 'cleaned_key': cleaned_key,'value': value, 'UUID': uuid})\n",
    "\n",
    "            # Increment the counter for this attribute field\n",
    "            field_counters[element.tag] += 1\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame and return\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def get_unique_tags(elements):\n",
    "    \"\"\"Gets a list of unique tag names from xml elements.\n",
    "\n",
    "    Args:\n",
    "        elements (list): List of XML elements.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique tag names.\n",
    "    \"\"\"\n",
    "    return list(set([element.tag for element in elements if element.attrib]))\n",
    "\n",
    "\n",
    "def create_dataframes(elements, tag_list, metadata_df):\n",
    "    \"\"\"Creates a dictionary of DataFrames for each unique XML tag.\n",
    "\n",
    "    Args:\n",
    "        elements (list): List of XML elements.\n",
    "        tag_list (list): List of unique XML tags.\n",
    "        metadata_df (pandas.DataFrame): DataFrame containing XML metadata.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing a DataFrame for each unique XML tag.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the dataframes\n",
    "    cfdi_df = {}\n",
    "\n",
    "    # Define key fields\n",
    "    uuid = get_attribute_cfdi(elements, 'TimbreFiscalDigital', 'UUID')\n",
    "    emisor_rfc = get_attribute_cfdi(elements, 'Emisor', 'Rfc')\n",
    "    receptor_rfc = get_attribute_cfdi(elements, 'Receptor', 'Rfc')\n",
    "    comprobante_tipo = get_attribute_cfdi(elements, 'Comprobante', 'TipoDeComprobante')   \n",
    "    \n",
    "    for tag in tag_list:\n",
    "        # Filter the DataFrame\n",
    "        filtered_df = metadata_df[metadata_df['tag'] == tag]\n",
    "        \n",
    "        # Custom aggregation function to concatenate values into a list\n",
    "        aggfunc = lambda x: list(x) if len(x) > 1 else np.max(x)\n",
    "        \n",
    "        # Create a pivot table\n",
    "        pivot_table = pd.pivot_table(filtered_df, values='value', index=['field_number'], columns=['cleaned_key'], aggfunc=aggfunc)\n",
    "\n",
    "        # Reset index and change the column names\n",
    "        df = pivot_table.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "        # Add the UUID and other key fields as new columns to the DataFrame\n",
    "        df['uuid'] = uuid \n",
    "        df['emisor_rfc'] = emisor_rfc \n",
    "        df['receptor_rfc'] = receptor_rfc\n",
    "        df['comprobante_tipo'] = comprobante_tipo\n",
    "\n",
    "        # Add the DataFrame to the dictionary\n",
    "        cfdi_df[tag] = df\n",
    "\n",
    "    return cfdi_df\n",
    "\n",
    "\n",
    "def create_db_engine(driver, server, port, database, username, password):\n",
    "    \"\"\"Create a connection engine for a database.\n",
    "\n",
    "    Args:\n",
    "        driver (str): The name of the ODBC driver for the database.\n",
    "        server (str): The address of the server hosting the database.\n",
    "        port (str): The port number to use to connect to the server.\n",
    "        database (str): The name of the database.\n",
    "        username (str): The username to use to connect to the database.\n",
    "        password (str): The password to use to connect to the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: The SQLAlchemy engine object.\n",
    "    \"\"\"\n",
    "    params = urllib.parse.quote_plus(\n",
    "        f'DRIVER={driver};SERVER={server},{port};DATABASE={database};UID={username};PWD={password}'\n",
    "    )\n",
    "    engine = create_engine(f'mssql+pyodbc:///?odbc_connect={params}')\n",
    "    return engine\n",
    "\n",
    "\n",
    "def write_to_db(df_dict, engine):\n",
    "    \"\"\"Writes DataFrames to a SQL database.\n",
    "\n",
    "    Args:\n",
    "        df_dict (dict): Dictionary where the key is the tag name and the value is the DataFrame to write to the database.\n",
    "        engine (sqlalchemy.engine.Engine): SQLAlchemy engine instance used to connect to the database.\n",
    "    \"\"\"\n",
    "    for tag, df in df_dict.items():\n",
    "        cleaned_tag = re.sub(r\"{.*?}\", \"\", tag)\n",
    "        table_name = 'cfdi_' + cleaned_tag\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "def get_xml_files(path, include_subdirectories=False):\n",
    "    xml_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                xml_files.append(os.path.join(root, file))\n",
    "        if not include_subdirectories:\n",
    "            break  # prevent os.walk() from traversing subdirectories\n",
    "\n",
    "    return xml_files\n",
    "\n",
    "\n",
    "def get_child_elements_from_path(file_path):\n",
    "    \"\"\"Parse an XML file and return all child elements.\"\"\"\n",
    "    xml_tree = ET.parse(file_path)\n",
    "    xml_root = xml_tree.getroot()\n",
    "    elements = get_child_elements(xml_root)\n",
    "    return elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_metadata_from_path(path, include_subdirectories=True):\n",
    "    xml_files = get_xml_files(path, include_subdirectories)\n",
    "\n",
    "    data = []\n",
    "    log = []\n",
    "    tag_list = set()  # change to a set to automatically eliminate duplicates\n",
    "    key_fields_list = []  # initialize an empty list to store key fields\n",
    "    elements_list = []   # initialize an empty list to store all elements\n",
    "\n",
    "    for xml_file in xml_files:\n",
    "        # Define a dictionary for the attribute field counters\n",
    "        field_counters = {}\n",
    "\n",
    "        #keyfields\n",
    "        file_name = os.path.basename(xml_file)\n",
    "        comprobante_tipo = get_attribute_cfdi_from_path(xml_file, 'Comprobante', 'TipoDeComprobante')\n",
    "        uuid = get_attribute_cfdi_from_path(xml_file, 'TimbreFiscalDigital', 'UUID')\n",
    "        emisor_rfc = get_attribute_cfdi_from_path(xml_file, 'Emisor', 'Rfc')\n",
    "        receptor_rfc = get_attribute_cfdi_from_path(xml_file, 'Receptor', 'Rfc')\n",
    "\n",
    "        # Pack the key fields into a dictionary and append it to the list\n",
    "        key_fields_list.append({\"file_name\": file_name, \"comprobante_tipo\": comprobante_tipo, \"uuid\": uuid, \"emisor_rfc\": emisor_rfc, \"receptor_rfc\": receptor_rfc})\n",
    "\n",
    "        # Get elements from the current xml file\n",
    "        elements = get_child_elements_from_path(xml_file)\n",
    "        elements_list.extend(elements)  # add elements to the elements_list\n",
    "\n",
    "        # Iterate over elements\n",
    "        for element in elements:\n",
    "            if element.attrib:\n",
    "                cleaned_tag = pascal_case(re.sub(r\"{.*?}\", \"\", element.tag)).lower()\n",
    "                tag_list.add(cleaned_tag)  # add the cleaned_tag to the tag_list set\n",
    "\n",
    "                # If this tag has not been seen before, initialize its counter\n",
    "                if element.tag not in field_counters:\n",
    "                    field_counters[element.tag] = 1\n",
    "\n",
    "                # Iterate over attributes of the element and add the data to the list\n",
    "                for key, value in element.attrib.items():\n",
    "                    cleaned_key = pascal_case(re.sub(r\"{.*?}\", \"\", key)).lower()\n",
    "                    data.append({'row': field_counters[element.tag], 'comprobante_tipo': comprobante_tipo, 'emisor_rfc': emisor_rfc, 'receptor_rfc': receptor_rfc, 'uuid': uuid, 'tag': element.tag, 'key': key,  'dataframe_name': cleaned_tag, 'field_name': cleaned_key,'field_value': value,'file_path': xml_file, 'file_name': file_name })\n",
    "\n",
    "                # Increment the counter for this attribute field\n",
    "                field_counters[element.tag] += 1\n",
    "\n",
    "        #log - record log info as a dictionary for each xml file\n",
    "        log.append({'comprobante_tipo': comprobante_tipo, 'emisor_rfc': emisor_rfc, 'receptor_rfc': receptor_rfc, 'file_name': file_name, 'uuid': uuid})\n",
    "\n",
    "    # create DataFrames after finishing the loop\n",
    "    metadata_df = pd.DataFrame(data)\n",
    "    log_df = pd.DataFrame(log)  # create DataFrame from log list\n",
    "\n",
    "    return metadata_df, log_df, list(tag_list), key_fields_list, elements_list  # return elements_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Roberto\\OneDrive\\cargoIabono\\Proyectos y Desarrollos\\P001_V001_CFDI-Reader\\01_Inputs\"\n",
    "\n",
    "metadata_df, log_df, tag_list, key_fields_list, elements_list = get_xml_metadata_from_path(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comprobante_tipo</th>\n",
       "      <th>emisor_rfc</th>\n",
       "      <th>receptor_rfc</th>\n",
       "      <th>file_name</th>\n",
       "      <th>uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>FIS780810KQ9</td>\n",
       "      <td>MES880922JQA</td>\n",
       "      <td>01DAD1D3-CA8F-4F0D-8CA7-E0347D111EC4.xml</td>\n",
       "      <td>01DAD1D3-CA8F-4F0D-8CA7-E0347D111EC4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>FIS780810KQ9</td>\n",
       "      <td>TCA640201ER3</td>\n",
       "      <td>0A8BEAE0-F410-4A97-8860-7F5EBADC0D60.xml</td>\n",
       "      <td>0A8BEAE0-F410-4A97-8860-7F5EBADC0D60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>FIS780810KQ9</td>\n",
       "      <td>KLM970416U59</td>\n",
       "      <td>0C58F816-7800-44E2-96FF-631A8E432C50.xml</td>\n",
       "      <td>0C58F816-7800-44E2-96FF-631A8E432C50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>FIS780810KQ9</td>\n",
       "      <td>PSM101118J20</td>\n",
       "      <td>0D0E1EBB-003E-4F17-B2BE-0A5442819D1C.xml</td>\n",
       "      <td>0D0E1EBB-003E-4F17-B2BE-0A5442819D1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P</td>\n",
       "      <td>FIS780810KQ9</td>\n",
       "      <td>FAN540305I15</td>\n",
       "      <td>0F23FE0D-8324-4BCE-AFAC-68CB67E89714.xml</td>\n",
       "      <td>0F23FE0D-8324-4BCE-AFAC-68CB67E89714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>FIS780810KQ9</td>\n",
       "      <td>CAT051122H76</td>\n",
       "      <td>1F8FA056-C516-4349-8E87-DC2EC8F831D5.xml</td>\n",
       "      <td>1F8FA056-C516-4349-8E87-DC2EC8F831D5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>N</td>\n",
       "      <td>EMS2103108P3</td>\n",
       "      <td>AOAR951019842</td>\n",
       "      <td>EMS2103108P3_Pago de nómina_20220815_N_AOAR951...</td>\n",
       "      <td>B7D29180-3BB4-904C-9DC5-8730016AA924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comprobante_tipo    emisor_rfc   receptor_rfc  \\\n",
       "0                I  FIS780810KQ9   MES880922JQA   \n",
       "1                I  FIS780810KQ9   TCA640201ER3   \n",
       "2                I  FIS780810KQ9   KLM970416U59   \n",
       "3                I  FIS780810KQ9   PSM101118J20   \n",
       "4                P  FIS780810KQ9   FAN540305I15   \n",
       "5                I  FIS780810KQ9   CAT051122H76   \n",
       "6                N  EMS2103108P3  AOAR951019842   \n",
       "\n",
       "                                           file_name  \\\n",
       "0           01DAD1D3-CA8F-4F0D-8CA7-E0347D111EC4.xml   \n",
       "1           0A8BEAE0-F410-4A97-8860-7F5EBADC0D60.xml   \n",
       "2           0C58F816-7800-44E2-96FF-631A8E432C50.xml   \n",
       "3           0D0E1EBB-003E-4F17-B2BE-0A5442819D1C.xml   \n",
       "4           0F23FE0D-8324-4BCE-AFAC-68CB67E89714.xml   \n",
       "5           1F8FA056-C516-4349-8E87-DC2EC8F831D5.xml   \n",
       "6  EMS2103108P3_Pago de nómina_20220815_N_AOAR951...   \n",
       "\n",
       "                                   uuid  \n",
       "0  01DAD1D3-CA8F-4F0D-8CA7-E0347D111EC4  \n",
       "1  0A8BEAE0-F410-4A97-8860-7F5EBADC0D60  \n",
       "2  0C58F816-7800-44E2-96FF-631A8E432C50  \n",
       "3  0D0E1EBB-003E-4F17-B2BE-0A5442819D1C  \n",
       "4  0F23FE0D-8324-4BCE-AFAC-68CB67E89714  \n",
       "5  1F8FA056-C516-4349-8E87-DC2EC8F831D5  \n",
       "6  B7D29180-3BB4-904C-9DC5-8730016AA924  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01DAD1D3-CA8F-4F0D-8CA7-E0347D111EC4\n",
      "0A8BEAE0-F410-4A97-8860-7F5EBADC0D60\n",
      "0C58F816-7800-44E2-96FF-631A8E432C50\n",
      "0D0E1EBB-003E-4F17-B2BE-0A5442819D1C\n",
      "0F23FE0D-8324-4BCE-AFAC-68CB67E89714\n",
      "1F8FA056-C516-4349-8E87-DC2EC8F831D5\n",
      "B7D29180-3BB4-904C-9DC5-8730016AA924\n"
     ]
    }
   ],
   "source": [
    "for key_fields in key_fields_list:\n",
    "    print(key_fields['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes2(metadata_df):\n",
    "    # Unique comprobante_tipo values\n",
    "    unique_comprobante_tipos = metadata_df['comprobante_tipo'].unique()\n",
    "\n",
    "    # Create an empty dictionary to hold the dataframes\n",
    "    cfdi_df = {}\n",
    "\n",
    "    for comprobante_tipo in unique_comprobante_tipos:\n",
    "        # Filter the DataFrame by comprobante_tipo\n",
    "        filtered_df = metadata_df[metadata_df['comprobante_tipo'] == comprobante_tipo]\n",
    "\n",
    "        # Unique tags for each comprobante_tipo\n",
    "        unique_tags = filtered_df['dataframe_name'].unique()\n",
    "\n",
    "        cfdi_df[comprobante_tipo] = {}  # Initialize a dictionary for each comprobante_tipo\n",
    "\n",
    "        for tag in unique_tags:\n",
    "            # Filter the DataFrame by tag\n",
    "            tag_df = filtered_df[filtered_df['dataframe_name'] == tag]\n",
    "\n",
    "            # Custom aggregation function to concatenate values into a list\n",
    "            aggfunc = lambda x: list(x) if len(x) > 1 else np.max(x)\n",
    "\n",
    "            # Create a pivot table\n",
    "            pivot_table = pd.pivot_table(tag_df, values='field_value', index=['row', 'uuid', 'emisor_rfc', 'receptor_rfc', 'comprobante_tipo'], columns=['field_name'], aggfunc=aggfunc)\n",
    "\n",
    "            # Reset index and change the column names\n",
    "            df = pivot_table.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "            cfdi_df[comprobante_tipo][tag] = df\n",
    "\n",
    "    return cfdi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert uuid, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUntitled-3.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m cfdi_df \u001b[39m=\u001b[39m create_dataframes2(metadata_df)\n",
      "\u001b[1;32mUntitled-3.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=24'>25</a>\u001b[0m         pivot_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mpivot_table(tag_df, values\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfield_value\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mrow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39memisor_rfc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreceptor_rfc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcomprobante_tipo\u001b[39m\u001b[39m'\u001b[39m], columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfield_name\u001b[39m\u001b[39m'\u001b[39m], aggfunc\u001b[39m=\u001b[39maggfunc)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=26'>27</a>\u001b[0m         \u001b[39m# Reset index and change the column names\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=27'>28</a>\u001b[0m         df \u001b[39m=\u001b[39m pivot_table\u001b[39m.\u001b[39;49mreset_index()\u001b[39m.\u001b[39mrename_axis(\u001b[39mNone\u001b[39;00m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=29'>30</a>\u001b[0m         cfdi_df[comprobante_tipo][tag] \u001b[39m=\u001b[39m df\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cfdi_df\n",
      "File \u001b[1;32mc:\\Users\\Roberto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Roberto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:6350\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[1;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[0;32m   6344\u001b[0m         \u001b[39mif\u001b[39;00m lab \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   6345\u001b[0m             \u001b[39m# if we have the codes, extract the values with a mask\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m             level_values \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39mtake(\n\u001b[0;32m   6347\u001b[0m                 level_values, lab, allow_fill\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fill_value\u001b[39m=\u001b[39mlev\u001b[39m.\u001b[39m_na_value\n\u001b[0;32m   6348\u001b[0m             )\n\u001b[1;32m-> 6350\u001b[0m         new_obj\u001b[39m.\u001b[39;49minsert(\n\u001b[0;32m   6351\u001b[0m             \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   6352\u001b[0m             name,\n\u001b[0;32m   6353\u001b[0m             level_values,\n\u001b[0;32m   6354\u001b[0m             allow_duplicates\u001b[39m=\u001b[39;49mallow_duplicates,\n\u001b[0;32m   6355\u001b[0m         )\n\u001b[0;32m   6357\u001b[0m new_obj\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m new_index\n\u001b[0;32m   6358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n",
      "File \u001b[1;32mc:\\Users\\Roberto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4806\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   4800\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   4801\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot specify \u001b[39m\u001b[39m'\u001b[39m\u001b[39mallow_duplicates=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4802\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mself.flags.allows_duplicate_labels\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4803\u001b[0m     )\n\u001b[0;32m   4804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_duplicates \u001b[39mand\u001b[39;00m column \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m   4805\u001b[0m     \u001b[39m# Should this be a different kind of error??\u001b[39;00m\n\u001b[1;32m-> 4806\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot insert \u001b[39m\u001b[39m{\u001b[39;00mcolumn\u001b[39m}\u001b[39;00m\u001b[39m, already exists\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4807\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, \u001b[39mint\u001b[39m):\n\u001b[0;32m   4808\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mloc must be int\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot insert uuid, already exists"
     ]
    }
   ],
   "source": [
    "cfdi_df = create_dataframes2(metadata_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instrucciones**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  determinar el documto y armar loop basado en el tipo de cfdi y y la lista de archivos\n",
    "-  generar log de los archivos leidos, tipo de archivo, \n",
    "\n",
    "\n",
    "- modificar el write db, para permitir seleccionar el schema: cfdi, y permitir que el tipo de cfdi sea parte del nombre de la tabla como prefix, para tener tablas por tipo de documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
